{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7963778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import time\n",
    "import socket\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3504206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_res(values, title=''):   \n",
    "    ''' Plot the reward curve and histogram of results over time.'''\n",
    "    # Update the window after each episode\n",
    "    #clear_output(wait=True)\n",
    "    \n",
    "    # Define the figure\n",
    "    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "    f.suptitle(title)\n",
    "    ax[0].plot(values, label='score per run')\n",
    "    ax[0].axhline(500, c='red',ls='--', label='goal')\n",
    "    ax[0].set_xlabel('Episodes')\n",
    "    ax[0].set_ylabel('Reward')\n",
    "    ax[0].legend()\n",
    "    # Calculate the trend\n",
    "    try:\n",
    "        #print(values)\n",
    "        x = range(len(values))\n",
    "        #x = np.arange(1,len(values) + 1)\n",
    "        #print(x)\n",
    "        z = np.polyfit(x, values, 1)\n",
    "        #print(z, '\\n\\n')\n",
    "        p = np.poly1d(z)\n",
    "        ax[0].plot(x,p(x),\"--\", label='trend')\n",
    "    except:\n",
    "        print('Error')\n",
    "    \n",
    "    # Plot the histogram of results\n",
    "    ax[1].hist(values[-50:])\n",
    "    ax[1].axvline(500, c='red', label='goal')\n",
    "    ax[1].set_xlabel('Scores per Last 50 Episodes')\n",
    "    ax[1].set_ylabel('Frequency')\n",
    "    ax[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f303f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game():\n",
    "    def __init__(self, host=\"localhost\", port=38514, num_episodes = 100):\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.episodes = num_episodes\n",
    "        self.error_num = 1\n",
    "        try:\n",
    "            self.sock.bind((host, port))\n",
    "        except socket.error as err:\n",
    "            print('Bind failed. Error Code : ' .format(err))\n",
    "        \n",
    "    def connect(self):\n",
    "        self.sock.listen(1)\n",
    "        self.conn, _ = self.sock.accept()\n",
    "        self.conn.send(bytes(str(self.episodes) + \"\\n\",'UTF-8'))\n",
    "        \n",
    "    def get_state(self):\n",
    "        data = self.conn.recv(512)\n",
    "        data = data.decode(encoding='UTF-8')\n",
    "        #print(data)\n",
    "\n",
    "        try:\n",
    "            lista = data.split(\";\")\n",
    "            reward = int(lista[1])\n",
    "            action = int(lista[2])\n",
    "            if lista[0] == \"gameOver\":\n",
    "                self.conn.send(bytes(\"GAMEOVER\\n\",'UTF-8'))\n",
    "                return None, reward, action\n",
    "                \n",
    "            state_list = lista[0].split(\"/\")\n",
    "            \n",
    "            list_dist_pills = list(map(int, state_list[0].replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")))\n",
    "            list_dist_power_pills = list(map(int, state_list[1].replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")))\n",
    "            list_dist_ghosts = list(map(int, state_list[2].replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")))\n",
    "\n",
    "            \n",
    "            #state_list_int = [list(map(int, x[1:-1].split(\",\"))) for x in state_list]\n",
    "\n",
    "            #state_list_int is reescaled to the range [0,1]\n",
    "            \n",
    "\n",
    "            next_state = list_dist_pills + list_dist_power_pills + list_dist_ghosts\n",
    "            \n",
    "            max_num = 500\n",
    "            min_num = 0\n",
    "            #print(\"Antes\",next_state)\n",
    "\n",
    "            next_state = [(x - min_num)/(max_num - min_num) for x in next_state]\n",
    "            #print(\"Despues\",next_state)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            f = open(\"error_file.txt\" ,\"a+\")\n",
    "            f.write(str(self.error_num) + \": \" + data + \"\\n\")\n",
    "            f.close()\n",
    "            self.error_num += 1\n",
    "            next_state = [-38514, -38514, -38514, -38514, -38514, -38514, -38514, -38514, -38514, -38514, -38514, -38514]\n",
    "            reward = 0\n",
    "            action = 0\n",
    "        return next_state, reward, action\n",
    "    \n",
    "    def send_action(self, action1, action2):\n",
    "        self.conn.send(bytes(str(action1) + \";\" + str(action2) + \"\\n\",'UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46fb27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    ''' Deep Q Neural Network class. '''\n",
    "    def __init__(self, state_dim=12, action_dim=4, hidden_dim=8, lr=0.0005):\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.model = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(state_dim, hidden_dim),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(hidden_dim, action_dim))\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr) #cambiar\n",
    "        \n",
    "    def update(self, state, y):\n",
    "        \"\"\"Update the weights of the network given a training sample. \"\"\"\n",
    "        tensor = torch.Tensor(state)\n",
    "        y_pred = self.model(tensor)\n",
    "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def predict(self, state):\n",
    "        \"\"\" Compute Q values for all actions using the DQL. \"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.model(torch.Tensor(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0c66b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_replay(DQN):\n",
    "    def replay(self, memory, size=32, gamma=0.9):\n",
    "        \"\"\"New replay function\"\"\"\n",
    "        #Try to improve replay speed\n",
    "        if len(memory) >= size:\n",
    "            batch = random.sample(memory,size)\n",
    "            \n",
    "            batch_t = list(map(list, zip(*batch))) #Transpose batch list\n",
    "\n",
    "            states = batch_t[0]\n",
    "            actions = batch_t[1]\n",
    "            next_states = batch_t[2]\n",
    "            rewards = batch_t[3]\n",
    "            is_dones = batch_t[4]\n",
    "        \n",
    "            states = torch.Tensor(states)\n",
    "            actions_tensor = torch.Tensor(actions)\n",
    "            next_states = torch.Tensor(next_states)\n",
    "            rewards = torch.Tensor(rewards)\n",
    "            is_dones_tensor = torch.Tensor(is_dones)\n",
    "        \n",
    "            is_dones_indices = torch.where(is_dones_tensor==True)[0]\n",
    "        \n",
    "            all_q_values = self.model(states) # predicted q_values of all states\n",
    "            all_q_values_next = self.model(next_states)\n",
    "            #Update q values\n",
    "            all_q_values[range(len(all_q_values)),actions]=rewards+gamma*torch.max(all_q_values_next, axis=1).values\n",
    "            all_q_values[is_dones_indices.tolist(), actions_tensor[is_dones].tolist()]=rewards[is_dones_indices.tolist()]\n",
    "        \n",
    "            \n",
    "            self.update(states.tolist(), all_q_values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10d22f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(model, episodes=100, gamma=0.7, epsilon=0.2, title = 'DQN'):\n",
    "    \"\"\"Deep Q Learning algorithm using the DQN. \"\"\"\n",
    "    game = Game(num_episodes = episodes)\n",
    "    game.connect()\n",
    "    q_values = []\n",
    "    episode_i=0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        episode_i+=1\n",
    "                \n",
    "        # Reset state\n",
    "        state, _, _ = game.get_state()\n",
    "        \n",
    "        total = 0\n",
    "        \n",
    "        while True:\n",
    "            # Implement greedy search policy to explore the state space\n",
    "            if random.random() < epsilon:\n",
    "                action1 = random.randint(0,3)\n",
    "                action2 = action1+1 % 4\n",
    "                game.send_action(action1, action2)\n",
    "                \n",
    "            else:\n",
    "                q_values = model.predict(state)\n",
    "                prediction = torch.topk(q_values, k=2)\n",
    "                game.send_action(prediction[1].data[0].item(), prediction[1].data[1].item())\n",
    "                \n",
    "            # Take action and add reward to total\n",
    "            next_state, reward, action = game.get_state() \n",
    "            \n",
    "            if type(q_values) != list:\n",
    "                q_values = q_values.tolist()\n",
    "            else:\n",
    "                 q_values = model.predict(state).tolist()\n",
    "            \n",
    "            if next_state is None:\n",
    "                q_values[action] = reward\n",
    "                # Update network weights\n",
    "                model.update(state, q_values)\n",
    "                break\n",
    "            \n",
    "            q_values_next = model.predict(next_state)\n",
    "            q_values[action] = reward + gamma * torch.max(q_values_next).item()\n",
    "            model.update(state, q_values)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "        \n",
    "        #plot_res(final, title)\n",
    "        \n",
    "        #print(\"episode: {}, total reward: {}\".format(episode_i, total))\n",
    "        if episode % 1000:\n",
    "            torch.save(model, \"model\" + str(episode) + \".mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c5f20c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_replay(model, episodes=100, gamma=0.7, epsilon=0.2, replay_size=32, memory_size=10000, title='DQN Replay'):\n",
    "    \"\"\"Deep Q Learning algorithm using the DQN. \"\"\"\n",
    "    game = Game(num_episodes = episodes)\n",
    "    game.connect()\n",
    "    q_values = []\n",
    "    memory = []\n",
    "    episode_i = 0\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        episode_i += 1\n",
    "        \n",
    "        # Reset state\n",
    "        state, _, _ = game.get_state()\n",
    "\n",
    "        total = 0\n",
    "        \n",
    "        while True:\n",
    "            # Implement greedy search policy to explore the state space\n",
    "            if random.random() < epsilon:\n",
    "                action1 = random.randint(0,3)\n",
    "                action2 = (action1+1) % 4\n",
    "                game.send_action(action1, action2)\n",
    "            else:\n",
    "                q_values = model.predict(state)\n",
    "                prediction = torch.topk(q_values, k=2)\n",
    "                game.send_action(prediction[1].data[0].item(), prediction[1].data[1].item())\n",
    "\n",
    "                \n",
    "            # Take action and add reward to total\n",
    "            next_state, reward, action = game.get_state()\n",
    "            \n",
    "            # Update total\n",
    "            if type(q_values) != list:\n",
    "                q_values = q_values.tolist()\n",
    "            else:\n",
    "                 q_values = model.predict(state).tolist()\n",
    "\n",
    "            if next_state is None:\n",
    "                break\n",
    "            \n",
    "            #remove first element from memory\n",
    "            if len(memory) >= memory_size:\n",
    "                memory.pop(0)\n",
    "            memory.append((state, action, next_state, reward, next_state is None))\n",
    "            \n",
    "            model.replay(memory, replay_size, gamma)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        #plot_res(final, title)\n",
    "        \n",
    "        if (episode % 1000) ==0:\n",
    "            torch.save(model, \"model\" + str(episode) + \".mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae070280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid literal for int() with base 10: '2[12, 12, 24, 12]/[68, 68, 68, 68]/[500, 500, 500, 500]'\n",
      "invalid literal for int() with base 10: '1[4, 4, 500, 4]/[40, 40, 500, 76]/[500, 500, 500, 500]'\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "[WinError 10054] Se ha forzado la interrupci贸n de una conexi贸n existente por el host remoto",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-babed756f815>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQN_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mq_learning_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-74b56edf48bf>\u001b[0m in \u001b[0;36mq_learning_replay\u001b[1;34m(model, episodes, gamma, epsilon, replay_size, memory_size, title)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# Reset state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-d6cdae7ed41e>\u001b[0m in \u001b[0;36mget_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m#print(data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] Se ha forzado la interrupci贸n de una conexi贸n existente por el host remoto"
     ]
    }
   ],
   "source": [
    "model = DQN_replay()\n",
    "q_learning_replay(model, episodes=10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd09e33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
