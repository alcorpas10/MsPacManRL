{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6086d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import time\n",
    "import socket\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ab134a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    ''' Deep Q Neural Network class. '''\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=0.05):\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.model = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(state_dim, hidden_dim),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(hidden_dim, hidden_dim*2),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(hidden_dim*2, action_dim))\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr) #cambiar\n",
    "        \n",
    "    def update(self, state, y):\n",
    "        \"\"\"Update the weights of the network given a training sample. \"\"\"\n",
    "        tensor = torch.Tensor(state)\n",
    "        y_pred = self.model(tensor)\n",
    "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def predict(self, state):\n",
    "        \"\"\" Compute Q values for all actions using the DQL. \"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.model(torch.Tensor(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55407ca2",
   "metadata": {},
   "source": [
    "state_dim -> input red nuronal, depende del estado\n",
    "\n",
    "action_dim -> 4 acciones: up, down, left y right\n",
    "\n",
    "hidden_dim -> ajustar hiperparámetro\n",
    "\n",
    "lr -> ajustar hiperparámetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc0e75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = 4\n",
    "hidden_dim = 100\n",
    "lr = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffb5eb4",
   "metadata": {},
   "source": [
    "### Prueba preliminar\n",
    "Distancia a la pill más cercana en cada una de las direcciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "616e6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceca9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "gamma = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e74b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(state_dim, action_dim, hidden_dim, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e1d5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game():\n",
    "    def __init__(self, host=\"localhost\", port=38514, numEpisodes = 100):\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.episodes = numEpisodes\n",
    "        try:\n",
    "            self.sock.bind((host, port))\n",
    "        except socket.error as err:\n",
    "            print('Bind failed. Error Code : ' .format(err))\n",
    "        \n",
    "    def connect(self):\n",
    "        self.sock.listen(1)\n",
    "        self.conn, _ = self.sock.accept()\n",
    "        self.conn.send(bytes(str(self.episodes) + \"\\n\",'UTF-8'))\n",
    "        \n",
    "    def get_state(self):\n",
    "        data = self.conn.recv(512)\n",
    "        data = data.decode(encoding='UTF-8')\n",
    "        lista=data.split(\";\")\n",
    "        reward= int(lista[1])\n",
    "        #print(data)\n",
    "        if lista[0] == \"gameOver\":\n",
    "            return None, reward\n",
    "        next_state=list(map(int, lista[0].replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")))\n",
    "        return next_state,reward\n",
    "    \n",
    "    def send_action(self, action):\n",
    "        self.conn.send(bytes(str(action) + \"\\n\",'UTF-8'))\n",
    "    \n",
    "    \"\"\"def step(self,action):\n",
    "        self.conn.send(bytes(str(action) + \"\\n\",'UTF-8'))\n",
    "        data = self.conn.recv(512)\n",
    "        data = data.decode(encoding='UTF-8')\n",
    "        lista=data.split(\";\")\n",
    "        #print(data)\n",
    "        next_state=list(map(int, lista[0].replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")))\n",
    "        reward= int(lista[1])\n",
    "       \n",
    "        return next_state,reward\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c3b0d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_res(values, title=''):   \n",
    "    ''' Plot the reward curve and histogram of results over time.'''\n",
    "    # Update the window after each episode\n",
    "    #clear_output(wait=True)\n",
    "    \n",
    "    # Define the figure\n",
    "    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "    f.suptitle(title)\n",
    "    ax[0].plot(values, label='score per run')\n",
    "    ax[0].axhline(195, c='red',ls='--', label='goal')\n",
    "    ax[0].set_xlabel('Episodes')\n",
    "    ax[0].set_ylabel('Reward')\n",
    "    x = range(len(values))\n",
    "    ax[0].legend()\n",
    "    # Calculate the trend\n",
    "    try:\n",
    "        z = np.polyfit(x, values, 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax[0].plot(x,p(x),\"--\", label='trend')\n",
    "    except:\n",
    "        print('')\n",
    "    \n",
    "    # Plot the histogram of results\n",
    "    ax[1].hist(values[-50:])\n",
    "    ax[1].axvline(195, c='red', label='goal')\n",
    "    ax[1].set_xlabel('Scores per Last 50 Episodes')\n",
    "    ax[1].set_ylabel('Frequency')\n",
    "    ax[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66ee7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(model, episodes = 100, gamma=0.7, epsilon=0.3, title = 'DQN'):\n",
    "    \"\"\"Deep Q Learning algorithm using the DQN. \"\"\"\n",
    "    game = Game(numEpisodes = episodes)\n",
    "    game.connect()\n",
    "    final = []\n",
    "    episode_i=0\n",
    "    for episode in range(episodes):\n",
    "        episode_i+=1\n",
    "                \n",
    "        # Reset state\n",
    "        state, _ = game.get_state()\n",
    "        \n",
    "        total = 0\n",
    "        \n",
    "        while True:\n",
    "            # Implement greedy search policy to explore the state space\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randint(0,3)\n",
    "            else:\n",
    "                q_values = model.predict(state)\n",
    "                action = torch.argmax(q_values).item()\n",
    "                \n",
    "            # Take action and add reward to total\n",
    "            game.send_action(action)\n",
    "            next_state, reward = game.get_state()    \n",
    "           \n",
    "            \n",
    "            # Update total\n",
    "            total += reward\n",
    "            q_values = q_values.tolist()\n",
    "            \n",
    "            if next_state == None:\n",
    "                q_values[action] = reward\n",
    "                # Update network weights\n",
    "                model.update(state, q_values)\n",
    "                break\n",
    "            \n",
    "            q_values_next = model.predict(next_state)\n",
    "            q_values[action] = reward + gamma * torch.max(q_values_next).item()\n",
    "            model.update(state, q_values)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "        \n",
    "        final.append(total)\n",
    "        #plot_res(final, title)\n",
    "        \n",
    "        print(\"episode: {}, total reward: {}\".format(episode_i, total))\n",
    "            \n",
    "        #if total == 500:\n",
    "            #torch.save(model, \"model.mdl\")\n",
    "            #return final\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde7a4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_old(model, episodes = 100, gamma=0.7, epsilon=0.3, title = 'DQN'):\n",
    "    \"\"\"Deep Q Learning algorithm using the DQN. \"\"\"\n",
    "    game = Game(numEpisodes = episodes)\n",
    "    game.connect()\n",
    "    final = []\n",
    "    episode_i=0\n",
    "    for episode in range(episodes):\n",
    "        episode_i+=1\n",
    "                \n",
    "        # Reset state\n",
    "        state = game.get_state()\n",
    "        \n",
    "        done = False\n",
    "        total = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Implement greedy search policy to explore the state space\n",
    "            q_values = model.predict(state) #Devuelve array con probabilidades de accion\n",
    "            #action = torch.argmax(q_values).item() #Accion a realizar\n",
    "            # Implement greedy search policy to explore the state space\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randint(0,3)\n",
    "            else:\n",
    "                q_values = model.predict(state)\n",
    "                action = torch.argmax(q_values).item()\n",
    "                \n",
    "            # Take action and add reward to total\n",
    "            next_state, reward, done = game.step(action) #Quizas reward requiera tratamiento, \n",
    "            \n",
    "            #print(\"Action:\" , action)\n",
    "           #print(\"next_state\", next_state)\n",
    "            #print(\"reward\", reward)\n",
    "            #print(\"done\", done)\n",
    "            \n",
    "            # Update total and memory\n",
    "            total += reward\n",
    "            q_values = q_values.tolist()\n",
    "             \n",
    "            if done:\n",
    "                q_values[action] = reward\n",
    "                # Update network weights\n",
    "                model.update(state, q_values)\n",
    "                break\n",
    "                \n",
    "            # Update network weights using the last step only\n",
    "            q_values_next = model.predict(next_state)\n",
    "            q_values[action] = reward + gamma * torch.max(q_values_next).item()\n",
    "            model.update(state, q_values)\n",
    "            \n",
    "            state = game.get_state()\n",
    "            #state = next_state\n",
    "        \n",
    "        final.append(total)\n",
    "        #plot_res(final, title)\n",
    "        \n",
    "        print(\"episode: {}, total reward: {}\".format(episode_i, total))\n",
    "            \n",
    "        #if total == 500:\n",
    "            #torch.save(model, \"model.mdl\")\n",
    "            #return final\n",
    "    return final\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ac85a1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): data must be a sequence (got NoneType)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6528c85c8754>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mq_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-85a7f33d888f>\u001b[0m in \u001b[0;36mq_learning\u001b[1;34m(model, episodes, gamma, epsilon, title)\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[0mq_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[1;31m# Update network weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-0346df20a03b>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, state, y)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;34m\"\"\"Update the weights of the network given a training sample. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: new(): data must be a sequence (got NoneType)"
     ]
    }
   ],
   "source": [
    "q_learning(model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42fd1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
