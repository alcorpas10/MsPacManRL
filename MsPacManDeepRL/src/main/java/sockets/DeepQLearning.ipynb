{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6086d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import time\n",
    "import socket\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ab134a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    ''' Deep Q Neural Network class. '''\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=0.05):\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.model = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(state_dim, hidden_dim),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(hidden_dim, hidden_dim*2),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(hidden_dim*2, action_dim))\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr) #cambiar\n",
    "        \n",
    "    def update(self, state, y):\n",
    "        \"\"\"Update the weights of the network given a training sample. \"\"\"\n",
    "        tensor = torch.Tensor(state)\n",
    "        y_pred = self.model(tensor)\n",
    "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def predict(self, state):\n",
    "        \"\"\" Compute Q values for all actions using the DQL. \"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.model(torch.Tensor(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55407ca2",
   "metadata": {},
   "source": [
    "state_dim -> input red nuronal, depende del estado\n",
    "\n",
    "action_dim -> 4 acciones: up, down, left y right\n",
    "\n",
    "hidden_dim -> ajustar hiperparámetro\n",
    "\n",
    "lr -> ajustar hiperparámetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc0e75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = 4\n",
    "hidden_dim = 200\n",
    "lr = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "616e6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e74b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(state_dim, action_dim, hidden_dim, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffb5eb4",
   "metadata": {},
   "source": [
    "### Prueba preliminar\n",
    "Distancia a la pill más cercana en cada una de las direcciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceca9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "gamma = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e1d5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game():\n",
    "    def __init__(self, host=\"localhost\", port=38514, numEpisodes = 100):\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.episodes = numEpisodes\n",
    "        try:\n",
    "            self.sock.bind((host, port))\n",
    "        except socket.error as err:\n",
    "            print('Bind failed. Error Code : ' .format(err))\n",
    "        \n",
    "    def connect(self):\n",
    "        self.sock.listen(1)\n",
    "        self.conn, _ = self.sock.accept()\n",
    "        self.conn.send(bytes(str(self.episodes) + \"\\n\",'UTF-8'))\n",
    "        \n",
    "    def get_state(self):\n",
    "        data = self.conn.recv(512)\n",
    "        data = data.decode(encoding='UTF-8')\n",
    "        lista=data.split(\";\")\n",
    "        reward= int(lista[1])\n",
    "        #print(data)\n",
    "        if lista[0] == \"gameOver\":\n",
    "            return None, reward\n",
    "        next_state=list(map(int, lista[0].replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")))\n",
    "        return next_state,reward\n",
    "    \n",
    "    def send_action(self, action):\n",
    "        self.conn.send(bytes(str(action) + \"\\n\",'UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c3b0d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_res(values, title=''):   \n",
    "    ''' Plot the reward curve and histogram of results over time.'''\n",
    "    # Update the window after each episode\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Define the figure\n",
    "    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "    f.suptitle(title)\n",
    "    ax[0].plot(values, label='score per run')\n",
    "    ax[0].axhline(500, c='red',ls='--', label='goal')\n",
    "    ax[0].set_xlabel('Episodes')\n",
    "    ax[0].set_ylabel('Reward')\n",
    "    x = range(len(values))\n",
    "    ax[0].legend()\n",
    "    # Calculate the trend\n",
    "    try:\n",
    "        z = np.polyfit(x, values, 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax[0].plot(x,p(x),\"--\", label='trend')\n",
    "    except:\n",
    "        print('')\n",
    "    \n",
    "    # Plot the histogram of results\n",
    "    ax[1].hist(values[-50:])\n",
    "    ax[1].axvline(500, c='red', label='goal')\n",
    "    ax[1].set_xlabel('Scores per Last 50 Episodes')\n",
    "    ax[1].set_ylabel('Frequency')\n",
    "    ax[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66ee7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(model, episodes = 100, gamma=0.7, epsilon=0.3, title = 'DQN'):\n",
    "    \"\"\"Deep Q Learning algorithm using the DQN. \"\"\"\n",
    "    game = Game(numEpisodes = episodes)\n",
    "    game.connect()\n",
    "    final = []\n",
    "    q_values = []\n",
    "    episode_i=0\n",
    "    for episode in range(episodes):\n",
    "        episode_i+=1\n",
    "                \n",
    "        # Reset state\n",
    "        state, _ = game.get_state()\n",
    "        \n",
    "        total = 0\n",
    "        \n",
    "        while True:\n",
    "            # Implement greedy search policy to explore the state space\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randint(0,3)\n",
    "            else:\n",
    "                q_values = model.predict(state)\n",
    "                action = torch.argmax(q_values).item()\n",
    "                \n",
    "            # Take action and add reward to total\n",
    "            game.send_action(action)\n",
    "            next_state, reward = game.get_state()    \n",
    "           \n",
    "            \n",
    "            # Update total\n",
    "            total += reward\n",
    "            if type(q_values) != list:\n",
    "                q_values = q_values.tolist()\n",
    "            else:\n",
    "                 q_values = model.predict(state).tolist()\n",
    "            \n",
    "            if next_state is None:\n",
    "                q_values[action] = reward\n",
    "                # Update network weights\n",
    "                model.update(state, q_values)\n",
    "                break\n",
    "            \n",
    "            q_values_next = model.predict(next_state)\n",
    "            q_values[action] = reward + gamma * torch.max(q_values_next).item()\n",
    "            model.update(state, q_values)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "        \n",
    "        final.append(total)\n",
    "        #plot_res(final, title)\n",
    "        \n",
    "        #print(\"episode: {}, total reward: {}\".format(episode_i, total))\n",
    "            \n",
    "        #torch.save(model, \"model.mdl\")\n",
    "    return sum(final) / len(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7081ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_old(model, episodes = 100, gamma=0.7, epsilon=0.3, title = 'DQN'):\n",
    "    \"\"\"Deep Q Learning algorithm using the DQN. \"\"\"\n",
    "    game = Game(numEpisodes = episodes)\n",
    "    game.connect()\n",
    "    final = []\n",
    "    episode_i=0\n",
    "    for episode in range(episodes):\n",
    "        episode_i+=1\n",
    "                \n",
    "        # Reset state\n",
    "        state = game.get_state()\n",
    "        \n",
    "        done = False\n",
    "        total = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Implement greedy search policy to explore the state space\n",
    "            q_values = model.predict(state) #Devuelve array con probabilidades de accion\n",
    "            #action = torch.argmax(q_values).item() #Accion a realizar\n",
    "            # Implement greedy search policy to explore the state space\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randint(0,3)\n",
    "            else:\n",
    "                q_values = model.predict(state)\n",
    "                action = torch.argmax(q_values).item()\n",
    "                \n",
    "            # Take action and add reward to total\n",
    "            next_state, reward, done = game.step(action) #Quizas reward requiera tratamiento, \n",
    "            \n",
    "            #print(\"Action:\" , action)\n",
    "           #print(\"next_state\", next_state)\n",
    "            #print(\"reward\", reward)\n",
    "            #print(\"done\", done)\n",
    "            \n",
    "            # Update total and memory\n",
    "            total += reward\n",
    "            q_values = q_values.tolist()\n",
    "             \n",
    "            if done:\n",
    "                q_values[action] = reward\n",
    "                # Update network weights\n",
    "                model.update(state, q_values)\n",
    "                break\n",
    "                \n",
    "            # Update network weights using the last step only\n",
    "            q_values_next = model.predict(next_state)\n",
    "            q_values[action] = reward + gamma * torch.max(q_values_next).item()\n",
    "            model.update(state, q_values)\n",
    "            \n",
    "            state = game.get_state()\n",
    "            #state = next_state\n",
    "        \n",
    "        final.append(total)\n",
    "        #plot_res(final, title)\n",
    "        \n",
    "        #print(\"episode: {}, total reward: {}\".format(episode_i, total))\n",
    "            \n",
    "        #if total == 500:\n",
    "            #torch.save(model, \"model.mdl\")\n",
    "            #return final\n",
    "    return final\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ac85a1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#q_learning(model, episodes=30, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db3480ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_replay(DQN):\n",
    "    def replay(self, memory, size, gamma=0.9):\n",
    "        \"\"\"New replay function\"\"\"\n",
    "        #Try to improve replay speed\n",
    "        if len(memory) >= size:\n",
    "            batch = random.sample(memory,size)\n",
    "            \n",
    "            batch_t = list(map(list, zip(*batch))) #Transpose batch list\n",
    "\n",
    "            states = batch_t[0]\n",
    "            actions = batch_t[1]\n",
    "            next_states = batch_t[2]\n",
    "            rewards = batch_t[3]\n",
    "            is_dones = batch_t[4]\n",
    "        \n",
    "            states = torch.Tensor(states)\n",
    "            actions_tensor = torch.Tensor(actions)\n",
    "            next_states = torch.Tensor(next_states)\n",
    "            rewards = torch.Tensor(rewards)\n",
    "            is_dones_tensor = torch.Tensor(is_dones)\n",
    "        \n",
    "            is_dones_indices = torch.where(is_dones_tensor==True)[0]\n",
    "        \n",
    "            all_q_values = self.model(states) # predicted q_values of all states\n",
    "            all_q_values_next = self.model(next_states)\n",
    "            #Update q values\n",
    "            all_q_values[range(len(all_q_values)),actions]=rewards+gamma*torch.max(all_q_values_next, axis=1).values\n",
    "            all_q_values[is_dones_indices.tolist(), actions_tensor[is_dones].tolist()]=rewards[is_dones_indices.tolist()]\n",
    "        \n",
    "            \n",
    "            self.update(states.tolist(), all_q_values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46cd0713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_replay(model, episodes=100, gamma=0.7, epsilon=0.3, replay_size=20, title='DQN Replay'):\n",
    "    \"\"\"Deep Q Learning algorithm using the DQN. \"\"\"\n",
    "    game = Game(numEpisodes = episodes)\n",
    "    game.connect()\n",
    "    final = []\n",
    "    q_values = []\n",
    "    memory = []\n",
    "    episode_i=0\n",
    "    for episode in range(episodes):\n",
    "        episode_i+=1\n",
    "        intersecciones = 1     \n",
    "        # Reset state\n",
    "        state, _ = game.get_state()\n",
    "        \n",
    "        total = 0\n",
    "        \n",
    "        while True:\n",
    "            # Implement greedy search policy to explore the state space\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randint(0,3)\n",
    "            else:\n",
    "                q_values = model.predict(state)\n",
    "                action = torch.argmax(q_values).item()\n",
    "                \n",
    "            # Take action and add reward to total\n",
    "            game.send_action(action)\n",
    "            next_state, reward = game.get_state()\n",
    "            intersecciones += 1\n",
    "            # Update total\n",
    "            total += reward\n",
    "            if type(q_values) != list:\n",
    "                q_values = q_values.tolist()\n",
    "            else:\n",
    "                 q_values = model.predict(state).tolist()\n",
    "\n",
    "            if next_state is None:\n",
    "                break\n",
    "\n",
    "            memory.append((state, action, next_state, reward, next_state is None))\n",
    "            \n",
    "            model.replay(memory, replay_size, gamma)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        final.append(total)\n",
    "        #plot_res(final, title)\n",
    "        \n",
    "        #print(\"episode: {}, total reward: {}\".format(episode_i, total))\n",
    "            \n",
    "        torch.save(model, \"model.mdl\")\n",
    "    return sum(final)/len(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "363c24a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN_replay(state_dim, action_dim, hidden_dim, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e26d1231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_learning_replay(model, episodes=40, epsilon=0.1, replay_size=340)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc26e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_learning(model, episodes=40, epsilon=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd3de5a",
   "metadata": {},
   "source": [
    "Hiperparámetros a ajustar:\n",
    "* Learning rate\n",
    "* Hidden dimension\n",
    "* Number of episodes\n",
    "* Epsilon\n",
    "* Gamma\n",
    "\n",
    "Específica del replay DQN\n",
    "* Replay size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8ab6767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_parameters(episodes=(100, 1000, 10), hidden_layers=(100, 1000, 10), epsilon=(0.01, 0.1, 10), gamma=(0.5, 1, 10), lr=(0.01, 0.1, 5)):\n",
    "    means = []\n",
    "\n",
    "    episodes_list = np.linspace(episodes[0], episodes[1], episodes[2]).astype(int).tolist()\n",
    "    hidden_layers_list = np.linspace(hidden_layers[0], hidden_layers[1], hidden_layers[2]).astype(int).tolist()\n",
    "    epsilon_list = np.linspace(epsilon[0], epsilon[1], epsilon[2]).tolist()\n",
    "    gamma_list = np.linspace(gamma[0], gamma[1], gamma[2]).tolist()\n",
    "    lr_list = np.linspace(lr[0], lr[1], lr[2]).tolist()\n",
    "\n",
    "    for episode in episodes_list:\n",
    "        for hidden_layer in hidden_layers_list:\n",
    "            for eps in epsilon_list:\n",
    "                for gam in gamma_list:\n",
    "                    for l in lr_list:\n",
    "                        model = DQN(4, 4, hidden_layer, l)\n",
    "                        mean = q_learning(model, episodes=episode, gamma=gam, epsilon=eps)\n",
    "                        print(mean)\n",
    "                        means.append((mean, episode, hidden_layer, eps, gam, l))\n",
    "\n",
    "    return means\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57f0644b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582.51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(582.51, 1000, 1000, 0.1, 1, 0.1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjust_parameters(episodes=(100, 1000, 2), hidden_layers=(100, 1000, 2), epsilon=(0.01, 0.1, 2), gamma=(0.5, 1, 2), lr=(0.01, 0.1, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
