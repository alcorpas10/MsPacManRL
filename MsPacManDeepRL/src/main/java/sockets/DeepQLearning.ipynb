{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da805575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "import math\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import time\n",
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccaf08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    ''' Deep Q Neural Network class. '''\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=0.05):\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.model = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(state_dim, hidden_dim),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(hidden_dim, hidden_dim*2),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(hidden_dim*2, action_dim))\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr) #cambiar\n",
    "        \n",
    "    def update(self, state, y):\n",
    "        \"\"\"Update the weights of the network given a training sample. \"\"\"\n",
    "        tensor = torch.Tensor(state)\n",
    "        y_pred = self.model(tensor)\n",
    "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def predict(self, state):\n",
    "        \"\"\" Compute Q values for all actions using the DQL. \"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.model(torch.Tensor(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73834198",
   "metadata": {},
   "source": [
    "state_dim -> input red nuronal, depende del estado\n",
    "\n",
    "action_dim -> 4 acciones: up, down, left y right\n",
    "\n",
    "hidden_dim -> ajustar hiperparámetro\n",
    "\n",
    "lr -> ajustar hiperparámetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "224fa5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = 4\n",
    "hidden_dim = 100\n",
    "lr = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425be77f",
   "metadata": {},
   "source": [
    "### Prueba preliminar\n",
    "Distancia a la pill más cercana en cada una de las direcciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d41e4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a5891",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "gamma = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "974417c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(state_dim, action_dim, hidden_dim, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370fe4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game():\n",
    "    def __init__(self, host=\"localhost\", port=38514):\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        try:\n",
    "            self.sock.bind((host, port))\n",
    "        except socket.error as err:\n",
    "            print('Bind failed. Error Code : ' .format(err))\n",
    "        \n",
    "    def connect():\n",
    "        self.sock.listen(1)\n",
    "        self.conn, _ = self.sock.accept()\n",
    "        \n",
    "    def get_state():\n",
    "        return self.conn.recv(512)\n",
    "        \n",
    "    def step(action):\n",
    "        self.conn.send(bytes(str(action) + \"\\n\",'UTF-8'))\n",
    "        data = self.conn.recv(512)\n",
    "        data = data.decode(encoding='UTF-8')\n",
    "        return data.split(\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee2aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b23012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(game, model, episodes, gamma=0.7, title = 'DQN'):\n",
    "    \"\"\"Deep Q Learning algorithm using the DQN. \"\"\"\n",
    "    final = []\n",
    "    episode_i=0\n",
    "    for episode in range(episodes):\n",
    "        episode_i+=1\n",
    "                \n",
    "        # Reset state\n",
    "        state = game.get_state()\n",
    "        done = False\n",
    "        total = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Implement greedy search policy to explore the state space\n",
    "            q_values = model.predict(state) #Devuelve array con probabilidades de accion\n",
    "            action = torch.argmax(q_values).item() #Accion a realizar\n",
    "            \n",
    "            # Take action and add reward to total\n",
    "            next_state, reward, done = game.step(action) #Quizas reward requiera tratamiento, \n",
    "            \n",
    "            # Update total and memory\n",
    "            total += reward\n",
    "            q_values = q_values.tolist()\n",
    "             \n",
    "            if done:\n",
    "                q_values[action] = reward\n",
    "                # Update network weights\n",
    "                model.update(state, q_values)\n",
    "                break\n",
    "                \n",
    "            # Update network weights using the last step only\n",
    "            q_values_next = model.predict(next_state)\n",
    "            q_values[action] = reward + gamma * torch.max(q_values_next).item()\n",
    "            model.update(state, q_values)\n",
    "\n",
    "            state = next_state\n",
    "        \n",
    "        final.append(total)\n",
    "        plot_res(final, title)\n",
    "        \n",
    "        print(\"episode: {}, total reward: {}\".format(episode_i, total))\n",
    "            \n",
    "        #if total == 500:\n",
    "            #torch.save(model, \"model.mdl\")\n",
    "            #return final\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b25c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_res(values, title=''):   \n",
    "    ''' Plot the reward curve and histogram of results over time.'''\n",
    "    # Update the window after each episode\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Define the figure\n",
    "    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "    f.suptitle(title)\n",
    "    ax[0].plot(values, label='score per run')\n",
    "    ax[0].axhline(195, c='red',ls='--', label='goal')\n",
    "    ax[0].set_xlabel('Episodes')\n",
    "    ax[0].set_ylabel('Reward')\n",
    "    x = range(len(values))\n",
    "    ax[0].legend()\n",
    "    # Calculate the trend\n",
    "    try:\n",
    "        z = np.polyfit(x, values, 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax[0].plot(x,p(x),\"--\", label='trend')\n",
    "    except:\n",
    "        print('')\n",
    "    \n",
    "    # Plot the histogram of results\n",
    "    ax[1].hist(values[-50:])\n",
    "    ax[1].axvline(195, c='red', label='goal')\n",
    "    ax[1].set_xlabel('Scores per Last 50 Episodes')\n",
    "    ax[1].set_ylabel('Frequency')\n",
    "    ax[1].legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
