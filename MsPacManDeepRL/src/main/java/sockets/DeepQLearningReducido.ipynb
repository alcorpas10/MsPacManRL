{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7963778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import time\n",
    "import socket\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3504206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_res(values, title=''):   \n",
    "    ''' Plot the reward curve and histogram of results over time.'''\n",
    "    # Update the window after each episode\n",
    "    #clear_output(wait=True)\n",
    "    \n",
    "    # Define the figure\n",
    "    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "    f.suptitle(title)\n",
    "    ax[0].plot(values, label='score per run')\n",
    "    ax[0].axhline(500, c='red',ls='--', label='goal')\n",
    "    ax[0].set_xlabel('Episodes')\n",
    "    ax[0].set_ylabel('Reward')\n",
    "    ax[0].legend()\n",
    "    # Calculate the trend\n",
    "    try:\n",
    "        #print(values)\n",
    "        x = range(len(values))\n",
    "        #x = np.arange(1,len(values) + 1)\n",
    "        #print(x)\n",
    "        z = np.polyfit(x, values, 1)\n",
    "        #print(z, '\\n\\n')\n",
    "        p = np.poly1d(z)\n",
    "        ax[0].plot(x,p(x),\"--\", label='trend')\n",
    "    except:\n",
    "        print('Error')\n",
    "    \n",
    "    # Plot the histogram of results\n",
    "    ax[1].hist(values[-50:])\n",
    "    ax[1].axvline(500, c='red', label='goal')\n",
    "    ax[1].set_xlabel('Scores per Last 50 Episodes')\n",
    "    ax[1].set_ylabel('Frequency')\n",
    "    ax[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f303f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game():\n",
    "    def __init__(self, host=\"localhost\", port=38514, num_episodes = 100):\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.episodes = num_episodes\n",
    "        self.error_num = 1\n",
    "        try:\n",
    "            self.sock.bind((host, port))\n",
    "        except socket.error as err:\n",
    "            print('Bind failed. Error Code : ' .format(err))\n",
    "        \n",
    "    def connect(self):\n",
    "        self.sock.listen(1)\n",
    "        self.conn, _ = self.sock.accept()\n",
    "        self.conn.send(bytes(str(self.episodes) + \"\\n\",'UTF-8'))\n",
    "        \n",
    "    def get_state(self):\n",
    "        data = self.conn.recv(512)\n",
    "        data = data.decode(encoding='UTF-8')\n",
    "        print(data)\n",
    "\n",
    "        try:\n",
    "            lista = data.split(\";\")\n",
    "            reward = int(lista[1])\n",
    "            action = int(lista[2])\n",
    "            if lista[0] == \"gameOver\":\n",
    "                return None, reward, action\n",
    "                \n",
    "            lista_estado=lista[0].split(\"/\")\n",
    "            list_dist_pills = list(map(int, lista_estado[0].replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")))\n",
    "            list_dist_power_pills = list(map(int, lista_estado[1].replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")))\n",
    "            list_dist_ghosts = list(map(int, lista_estado[2].replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")))\n",
    "            \n",
    "            next_state = list_dist_pills + list_dist_power_pills + list_dist_ghosts\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            f = open(\"error_file.txt\" ,\"a+\")\n",
    "            f.write(str(self.error_num) + \": \" + data + \"\\n\")\n",
    "            f.close()\n",
    "            self.error_num += 1\n",
    "            next_state = [-38514, -38514, -38514, -38514, -38514, -38514, -38514, -38514, -38514, -38514, -38514, -38514]\n",
    "            reward = 0\n",
    "            action = 0\n",
    "        return next_state, reward, action\n",
    "    \n",
    "    def send_action(self, action1, action2):\n",
    "        self.conn.send(bytes(str(action1) + \";\" + str(action2) + \"\\n\",'UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46fb27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    ''' Deep Q Neural Network class. '''\n",
    "    def __init__(self, state_dim=12, action_dim=4, hidden_dim=8, lr=0.0005):\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.model = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(state_dim, hidden_dim),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(hidden_dim, action_dim))\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr) #cambiar\n",
    "        \n",
    "    def update(self, state, y):\n",
    "        \"\"\"Update the weights of the network given a training sample. \"\"\"\n",
    "        tensor = torch.Tensor(state)\n",
    "        y_pred = self.model(tensor)\n",
    "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def predict(self, state):\n",
    "        \"\"\" Compute Q values for all actions using the DQL. \"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.model(torch.Tensor(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0c66b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_replay(DQN):\n",
    "    def replay(self, memory, size=32, gamma=0.9):\n",
    "        \"\"\"New replay function\"\"\"\n",
    "        #Try to improve replay speed\n",
    "        if len(memory) >= size:\n",
    "            batch = random.sample(memory,size)\n",
    "            \n",
    "            batch_t = list(map(list, zip(*batch))) #Transpose batch list\n",
    "\n",
    "            states = batch_t[0]\n",
    "            actions = batch_t[1]\n",
    "            next_states = batch_t[2]\n",
    "            rewards = batch_t[3]\n",
    "            is_dones = batch_t[4]\n",
    "        \n",
    "            states = torch.Tensor(states)\n",
    "            actions_tensor = torch.Tensor(actions)\n",
    "            next_states = torch.Tensor(next_states)\n",
    "            rewards = torch.Tensor(rewards)\n",
    "            is_dones_tensor = torch.Tensor(is_dones)\n",
    "        \n",
    "            is_dones_indices = torch.where(is_dones_tensor==True)[0]\n",
    "        \n",
    "            all_q_values = self.model(states) # predicted q_values of all states\n",
    "            all_q_values_next = self.model(next_states)\n",
    "            #Update q values\n",
    "            all_q_values[range(len(all_q_values)),actions]=rewards+gamma*torch.max(all_q_values_next, axis=1).values\n",
    "            all_q_values[is_dones_indices.tolist(), actions_tensor[is_dones].tolist()]=rewards[is_dones_indices.tolist()]\n",
    "        \n",
    "            \n",
    "            self.update(states.tolist(), all_q_values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10d22f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(model, episodes=100, gamma=0.7, epsilon=0.2, title = 'DQN'):\n",
    "    \"\"\"Deep Q Learning algorithm using the DQN. \"\"\"\n",
    "    game = Game(num_episodes = episodes)\n",
    "    game.connect()\n",
    "    q_values = []\n",
    "    episode_i=0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        episode_i+=1\n",
    "                \n",
    "        # Reset state\n",
    "        state, _, _ = game.get_state()\n",
    "        \n",
    "        total = 0\n",
    "        \n",
    "        while True:\n",
    "            # Implement greedy search policy to explore the state space\n",
    "            if random.random() < epsilon:\n",
    "                action1 = random.randint(0,3)\n",
    "                action2 = action1+1 % 4\n",
    "                game.send_action(action1, action2)\n",
    "                \n",
    "            else:\n",
    "                q_values = model.predict(state)\n",
    "                prediction = torch.topk(q_values, k=2)\n",
    "                game.send_action(prediction[1].data[0].item(), prediction[1].data[1].item())\n",
    "                \n",
    "            # Take action and add reward to total\n",
    "            next_state, reward, action = game.get_state() \n",
    "            \n",
    "            if type(q_values) != list:\n",
    "                q_values = q_values.tolist()\n",
    "            else:\n",
    "                 q_values = model.predict(state).tolist()\n",
    "            \n",
    "            if next_state is None:\n",
    "                q_values[action] = reward\n",
    "                # Update network weights\n",
    "                model.update(state, q_values)\n",
    "                break\n",
    "            \n",
    "            q_values_next = model.predict(next_state)\n",
    "            q_values[action] = reward + gamma * torch.max(q_values_next).item()\n",
    "            print(q_values_next)\n",
    "            print(reward + gamma * torch.max(q_values_next).item())\n",
    "            model.update(state, q_values)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "        \n",
    "        #plot_res(final, title)\n",
    "        \n",
    "        #print(\"episode: {}, total reward: {}\".format(episode_i, total))\n",
    "        if episode % 1000:\n",
    "            torch.save(model, \"model\" + str(episode) + \".mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c5f20c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_replay(model, episodes=100, gamma=0.7, epsilon=0.2, replay_size=32, memory_size=10000, title='DQN Replay'):\n",
    "    \"\"\"Deep Q Learning algorithm using the DQN. \"\"\"\n",
    "    game = Game(num_episodes = episodes)\n",
    "    game.connect()\n",
    "    q_values = []\n",
    "    memory = []\n",
    "    episode_i = 0\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        episode_i += 1\n",
    "        \n",
    "        # Reset state\n",
    "        state, _, _ = game.get_state()\n",
    "\n",
    "        total = 0\n",
    "        \n",
    "        while True:\n",
    "            # Implement greedy search policy to explore the state space\n",
    "            if random.random() < epsilon:\n",
    "                action1 = random.randint(0,3)\n",
    "                action2 = action1+1 % 4\n",
    "                game.send_action(action1, action2)\n",
    "            else:\n",
    "                q_values = model.predict(state)\n",
    "                prediction = torch.topk(q_values, k=2)\n",
    "                game.send_action(prediction[1].data[0].item(), prediction[1].data[1].item())\n",
    "\n",
    "                \n",
    "            # Take action and add reward to total\n",
    "            next_state, reward, action = game.get_state()\n",
    "            \n",
    "            # Update total\n",
    "            if type(q_values) != list:\n",
    "                q_values = q_values.tolist()\n",
    "            else:\n",
    "                 q_values = model.predict(state).tolist()\n",
    "\n",
    "            if next_state is None:\n",
    "                break\n",
    "            \n",
    "            #remove first element from memory\n",
    "            if len(memory) >= memory_size:\n",
    "                memory.pop(0)\n",
    "            memory.append((state, action, next_state, reward, next_state is None))\n",
    "            \n",
    "            model.replay(memory, replay_size, gamma)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        #plot_res(final, title)\n",
    "        \n",
    "        if episode % 1000:\n",
    "            torch.save(model, \"model\" + str(episode) + \".mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae070280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 4, 500]/[44, 44, 72, 500]/[500, 500, 500, 500];-1;0\n",
      "[12, 12, 28, 12]/[68, 68, 68, 68]/[500, 500, 500, 500];-24;1\n",
      "[24, 500, 36, 24]/[80, 500, 84, 80]/[37, 500, 37, 37];-12;2\n",
      "[500, 24, 24, 24]/[500, 84, 84, 84]/[500, 55, 55, 55];-24;2\n",
      "[500, 4, 4, 4]/[500, 60, 60, 60]/[500, 55, 55, 72];-14;2\n",
      "[4, 4, 500, 4]/[48, 72, 500, 48]/[55, 55, 500, 55];18;2\n",
      "[500, 4, 4, 4]/[500, 52, 36, 36]/[500, 55, 55, 74];18;3\n",
      "[4, 4, 500, 4]/[28, 60, 500, 28]/[83, 83, 500, 83];36;2\n",
      "[4, 4, 500, 4]/[92, 92, 500, 92]/[5, 5, 500, 39];118;3\n",
      "[4, 4, 500, 28]/[88, 88, 500, 120]/[25, 25, 500, 25];-52;3\n",
      "[4, 4, 500, 4]/[92, 92, 500, 92]/[27, 61, 500, 27];-52;3\n",
      "[500, 4, 4, 4]/[500, 48, 16, 16]/[500, 500, 500, 500];-94;3\n",
      "[4, 4, 4, 4]/[28, 36, 28, 28]/[500, 500, 500, 500];-2;1\n",
      "[500, 4, 4, 32]/[500, 80, 56, 56]/[500, 42, 42, 42];42;2\n",
      "[16, 16, 28, 16]/[68, 68, 68, 68]/[22, 22, 22, 42];-12;2\n",
      "[0, 500, 0, 0]/[44, 500, 72, 44]/[500, 500, 500, 500];-117;1\n",
      "[24, 4, 4, 4]/[68, 48, 48, 48]/[500, 500, 500, 500];36;2\n",
      "[4, 500, 4, 4]/[80, 500, 36, 36]/[500, 500, 500, 500];18;2\n",
      "[4, 4, 500, 4]/[24, 24, 500, 64]/[81, 81, 500, 81];18;2\n",
      "[500, 4, 4, 4]/[500, 36, 36, 52]/[500, 81, 81, 81];18;3\n",
      "[4, 4, 500, 4]/[28, 28, 500, 60]/[87, 87, 500, 87];36;2\n",
      "[4, 4, 500, 4]/[40, 40, 500, 96]/[111, 111, 500, 133];18;3\n",
      "[4, 4, 500, 4]/[76, 76, 500, 108]/[65, 65, 500, 65];54;3\n",
      "[16, 16, 500, 28]/[88, 88, 500, 120]/[41, 41, 500, 41];8;3\n",
      "[4, 4, 500, 4]/[92, 92, 500, 92]/[41, 41, 500, 41];-52;3\n",
      "gameOver;0;3\n"
     ]
    }
   ],
   "source": [
    "model = DQN_replay()\n",
    "q_learning_replay(model, episodes=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
